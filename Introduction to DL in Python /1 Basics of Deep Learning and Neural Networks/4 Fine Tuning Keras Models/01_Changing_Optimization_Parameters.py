# # 8/7/2020
# It's time to get your hands dirty with optimization. You'll now try optimizing a model at a very low learning rate, a very high learning rate, and a "just right" learning rate. You'll want to look at the results after running this exercise, remembering that a low value for the loss function is good.

# For these exercises, we've pre-loaded the predictors and target values from your previous classification models (predicting who would survive on the Titanic). You'll want the optimization to start from scratch every time you change the learning rate, to give a fair comparison of how each learning rate did in your results. So we have created a function get_new_model() that creates an unoptimized model to optimize.

# Import the SGD optimizer
from keras.optimizers import SGD

# Create list of learning rates: lr_to_test
lr_to_test = [.000001, 0.01, 1]

# Loop over learning rates
for lr in lr_to_test:
    print('\n\nTesting model with learning rate: %f\n'%lr )
    
    # Build new model to test, unaffected by previous models
    model = get_new_model()
    
    # Create SGD optimizer with specified learning rate: my_optimizer
    my_optimizer = SGD(lr = lr)
    
    # Compile the model
    model.compile(optimizer = my_optimizer, loss = 'categorical_crossentropy')
    
    # Fit the model
    model.fit(predictors, target)
    

# <script.py> output:
    
    
#     Testing model with learning rate: 0.000001
    
#     Epoch 1/10
    
#  32/891 [>.............................] - ETA: 0s - loss: 3.6053
# 704/891 [======================>.......] - ETA: 0s - loss: 3.6753
# 891/891 [==============================] - 0s - loss: 3.6057     
#     Epoch 2/10
    
#  32/891 [>.............................] - ETA: 0s - loss: 3.5751
# 736/891 [=======================>......] - ETA: 0s - loss: 3.5123
# 891/891 [==============================] - 0s - loss: 3.5656     
#     Epoch 3/10
    
#  32/891 [>.............................] - ETA: 0s - loss: 2.6692
# 576/891 [==================>...........] - ETA: 0s - loss: 3.4286
# 891/891 [==============================] - 0s - loss: 3.5255     
#     Epoch 4/10
    
#  32/891 [>.............................] - ETA: 0s - loss: 3.0058
# 512/891 [================>.............] - ETA: 0s - loss: 3.4905
# 891/891 [==============================] - 0s - loss: 3.4854     
#     Epoch 5/10
    
#  32/891 [>.............................] - ETA: 0s - loss: 2.5452
# 704/891 [======================>.......] - ETA: 0s - loss: 3.4019
# 891/891 [==============================] - 0s - loss: 3.4454     
#     Epoch 6/10
    
#  32/891 [>.............................] - ETA: 0s - loss: 3.4446
# 704/891 [======================>.......] - ETA: 0s - loss: 3.4404
# 891/891 [==============================] - 0s - loss: 3.4056     
#     Epoch 7/10
    
#  32/891 [>.............................] - ETA: 0s - loss: 4.1073
# 640/891 [====================>.........] - ETA: 0s - loss: 3.4525
# 891/891 [==============================] - 0s - loss: 3.3659     
#     Epoch 8/10
    
#  32/891 [>.............................] - ETA: 0s - loss: 3.0972
# 704/891 [======================>.......] - ETA: 0s - loss: 3.2714
# 891/891 [==============================] - 0s - loss: 3.3263     
#     Epoch 9/10
    
#  32/891 [>.............................] - ETA: 0s - loss: 3.7464
# 704/891 [======================>.......] - ETA: 0s - loss: 3.2767
# 891/891 [==============================] - 0s - loss: 3.2867     
#     Epoch 10/10
    
#  32/891 [>.............................] - ETA: 0s - loss: 3.3862
# 736/891 [=======================>......] - ETA: 0s - loss: 3.1780
# 891/891 [==============================] - 0s - loss: 3.2473     
    
    
#     Testing model with learning rate: 0.010000
    
#     Epoch 1/10
    
#  32/891 [>.............................] - ETA: 1s - loss: 1.0910
# 672/891 [=====================>........] - ETA: 0s - loss: 1.6397
# 891/891 [==============================] - 0s - loss: 1.4069     
#     Epoch 2/10
    
#  32/891 [>.............................] - ETA: 0s - loss: 2.1145
# 736/891 [=======================>......] - ETA: 0s - loss: 0.7201
# 891/891 [==============================] - 0s - loss: 0.7036     
#     Epoch 3/10
    
#  32/891 [>.............................] - ETA: 0s - loss: 0.5716
# 704/891 [======================>.......] - ETA: 0s - loss: 0.6517
# 891/891 [==============================] - 0s - loss: 0.6469     
#     Epoch 4/10
    
#  32/891 [>.............................] - ETA: 0s - loss: 0.6275
# 704/891 [======================>.......] - ETA: 0s - loss: 0.6263
# 891/891 [==============================] - 0s - loss: 0.6175     
#     Epoch 5/10
    
#  32/891 [>.............................] - ETA: 0s - loss: 0.4938
# 736/891 [=======================>......] - ETA: 0s - loss: 0.6211
# 891/891 [==============================] - 0s - loss: 0.6242     
#     Epoch 6/10
    
#  32/891 [>.............................] - ETA: 0s - loss: 0.6611
# 736/891 [=======================>......] - ETA: 0s - loss: 0.6025
# 891/891 [==============================] - 0s - loss: 0.6002     
#     Epoch 7/10
    
#  32/891 [>.............................] - ETA: 0s - loss: 0.6244
# 736/891 [=======================>......] - ETA: 0s - loss: 0.6018
# 891/891 [==============================] - 0s - loss: 0.5980     
#     Epoch 8/10
    
#  32/891 [>.............................] - ETA: 0s - loss: 0.6077
# 736/891 [=======================>......] - ETA: 0s - loss: 0.5927
# 891/891 [==============================] - 0s - loss: 0.6025     
#     Epoch 9/10
    
#  32/891 [>.............................] - ETA: 0s - loss: 0.6535
# 736/891 [=======================>......] - ETA: 0s - loss: 0.5921
# 891/891 [==============================] - 0s - loss: 0.5915     
#     Epoch 10/10
    
#  32/891 [>.............................] - ETA: 0s - loss: 0.6415
# 736/891 [=======================>......] - ETA: 0s - loss: 0.5795
# 891/891 [==============================] - 0s - loss: 0.5818     
    
    
#     Testing model with learning rate: 1.000000
    
#     Epoch 1/10
    
#  32/891 [>.............................] - ETA: 1s - loss: 1.0273
# 512/891 [================>.............] - ETA: 0s - loss: 5.4474
# 891/891 [==============================] - 0s - loss: 5.9885     
#     Epoch 2/10
    
#  32/891 [>.............................] - ETA: 0s - loss: 4.5332
# 544/891 [=================>............] - ETA: 0s - loss: 6.1628
# 891/891 [==============================] - 0s - loss: 6.1867     
#     Epoch 3/10
    
#  32/891 [>.............................] - ETA: 0s - loss: 7.0517
# 576/891 [==================>...........] - ETA: 0s - loss: 5.9883
# 891/891 [==============================] - 0s - loss: 6.1867     
#     Epoch 4/10
    
#  32/891 [>.............................] - ETA: 0s - loss: 6.0443
# 704/891 [======================>.......] - ETA: 0s - loss: 6.1588
# 891/891 [==============================] - 0s - loss: 6.1867     
#     Epoch 5/10
    
#  32/891 [>.............................] - ETA: 0s - loss: 9.0664
# 704/891 [======================>.......] - ETA: 0s - loss: 6.0901
# 891/891 [==============================] - 0s - loss: 6.1867     
#     Epoch 6/10
    
#  32/891 [>.............................] - ETA: 0s - loss: 6.0443
# 704/891 [======================>.......] - ETA: 0s - loss: 6.1817
# 891/891 [==============================] - 0s - loss: 6.1867     
#     Epoch 7/10
    
#  32/891 [>.............................] - ETA: 0s - loss: 5.0369
# 704/891 [======================>.......] - ETA: 0s - loss: 6.2732
# 891/891 [==============================] - 0s - loss: 6.1867     
#     Epoch 8/10
    
#  32/891 [>.............................] - ETA: 0s - loss: 5.0369
# 704/891 [======================>.......] - ETA: 0s - loss: 6.0672
# 891/891 [==============================] - 0s - loss: 6.1867     
#     Epoch 9/10
    
#  32/891 [>.............................] - ETA: 0s - loss: 5.5406
# 704/891 [======================>.......] - ETA: 0s - loss: 6.0672
# 891/891 [==============================] - 0s - loss: 6.1867     
#     Epoch 10/10
    
#  32/891 [>.............................] - ETA: 0s - loss: 5.5406
# 704/891 [======================>.......] - ETA: 0s - loss: 6.2046
# 891/891 [==============================] - 0s - loss: 6.1867
# # 8/9/2020
# You've reached the final exercise of the course - you now know everything you need to build an accurate model to recognize handwritten digits!

# We've already done the basic manipulation of the MNIST dataset shown in the video, so you have X and y loaded and ready to model with. Sequential and Dense from keras are also pre-imported.

# To add an extra challenge, we've loaded only 2500 images, rather than 60000 which you will see in some published results. Deep learning models perform better with more data, however, they also take longer to train, especially when they start becoming more complex.

# If you have a computer with a CUDA compatible GPU, you can take advantage of it to improve computation time. If you don't have a GPU, no problem! You can set up a deep learning environment in the cloud that can run your models on a GPU. Here is a blog post by Dan that explains how to do this - check it out after completing this exercise! It is a great next step as you continue your deep learning journey.

# Create a Sequential object to start your model. Call this model.
# Add the first Dense hidden layer of 50 units to your model with 'relu' activation. For this data, the input_shape is (784,).
# Add a second Dense hidden layer with 50 units and a 'relu' activation function.
# Add the output layer. Your activation function should be 'softmax', and the number of nodes in this layer should be the same as the number of possible outputs in this case: 10.
# Compile model as you have done with previous models: Using 'adam' as the optimizer, 'categorical_crossentropy' for the loss, and metrics=['accuracy'].
# Fit the model using X and y using a validation_split of 0.3.

# Create the model: model
model = Sequential()

# Add the first hidden layer
model.add(Dense(50, activation= 'relu', input_shape = (784, )))

# Add the second hidden layer
model.add(Dense(50, activation= 'relu', input_shape = (784, )))

# Add the output layer
model.add(Dense(10, activation= 'softmax'))

# Compile the model
model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])

# Fit the model
model.fit(X, y, validation_split = 0.3)

# <script.py> output:
#     Train on 1750 samples, validate on 750 samples
#     Epoch 1/10
    
#   32/1750 [..............................] - ETA: 2s - loss: 2.1979 - acc: 0.2188
#  544/1750 [========>.....................] - ETA: 0s - loss: 2.1492 - acc: 0.2445
# 1056/1750 [=================>............] - ETA: 0s - loss: 1.9471 - acc: 0.3475
# 1568/1750 [=========================>....] - ETA: 0s - loss: 1.7498 - acc: 0.4369
# 1750/1750 [==============================] - 0s - loss: 1.6778 - acc: 0.4697 - val_loss: 1.0107 - val_acc: 0.7627
#     Epoch 2/10
    
#   32/1750 [..............................] - ETA: 0s - loss: 0.9546 - acc: 0.7500
#  544/1750 [========>.....................] - ETA: 0s - loss: 0.7557 - acc: 0.8327
# 1056/1750 [=================>............] - ETA: 0s - loss: 0.7363 - acc: 0.8229
# 1568/1750 [=========================>....] - ETA: 0s - loss: 0.6912 - acc: 0.8291
# 1750/1750 [==============================] - 0s - loss: 0.6767 - acc: 0.8291 - val_loss: 0.5338 - val_acc: 0.8627
#     Epoch 3/10
    
#   32/1750 [..............................] - ETA: 0s - loss: 0.3681 - acc: 0.9688
#  544/1750 [========>.....................] - ETA: 0s - loss: 0.4209 - acc: 0.8915
# 1088/1750 [=================>............] - ETA: 0s - loss: 0.4036 - acc: 0.8998
# 1568/1750 [=========================>....] - ETA: 0s - loss: 0.4184 - acc: 0.8890
# 1750/1750 [==============================] - 0s - loss: 0.4182 - acc: 0.8846 - val_loss: 0.4502 - val_acc: 0.8613
#     Epoch 4/10
    
#   32/1750 [..............................] - ETA: 0s - loss: 0.1816 - acc: 0.9375
#  288/1750 [===>..........................] - ETA: 0s - loss: 0.2783 - acc: 0.9271
#  672/1750 [==========>...................] - ETA: 0s - loss: 0.3165 - acc: 0.9182
# 1152/1750 [==================>...........] - ETA: 0s - loss: 0.3274 - acc: 0.9106
# 1632/1750 [==========================>...] - ETA: 0s - loss: 0.3285 - acc: 0.9062
# 1750/1750 [==============================] - 0s - loss: 0.3224 - acc: 0.9080 - val_loss: 0.3908 - val_acc: 0.8813
#     Epoch 5/10
    
#   32/1750 [..............................] - ETA: 0s - loss: 0.1709 - acc: 0.9375
#  480/1750 [=======>......................] - ETA: 0s - loss: 0.2683 - acc: 0.9271
#  896/1750 [==============>...............] - ETA: 0s - loss: 0.2670 - acc: 0.9275
# 1312/1750 [=====================>........] - ETA: 0s - loss: 0.2430 - acc: 0.9360
# 1750/1750 [==============================] - 0s - loss: 0.2580 - acc: 0.9297 - val_loss: 0.3682 - val_acc: 0.8960
#     Epoch 6/10
    
#   32/1750 [..............................] - ETA: 0s - loss: 0.0803 - acc: 1.0000
#  544/1750 [========>.....................] - ETA: 0s - loss: 0.1913 - acc: 0.9504
# 1056/1750 [=================>............] - ETA: 0s - loss: 0.1987 - acc: 0.9498
# 1568/1750 [=========================>....] - ETA: 0s - loss: 0.2048 - acc: 0.9477
# 1750/1750 [==============================] - 0s - loss: 0.2085 - acc: 0.9440 - val_loss: 0.3469 - val_acc: 0.8973
#     Epoch 7/10
    
#   32/1750 [..............................] - ETA: 0s - loss: 0.1729 - acc: 0.9375
#  544/1750 [========>.....................] - ETA: 0s - loss: 0.1550 - acc: 0.9669
# 1056/1750 [=================>............] - ETA: 0s - loss: 0.1633 - acc: 0.9659
# 1568/1750 [=========================>....] - ETA: 0s - loss: 0.1714 - acc: 0.9598
# 1750/1750 [==============================] - 0s - loss: 0.1715 - acc: 0.9594 - val_loss: 0.3303 - val_acc: 0.8960
#     Epoch 8/10
    
#   32/1750 [..............................] - ETA: 0s - loss: 0.0824 - acc: 1.0000
#  544/1750 [========>.....................] - ETA: 0s - loss: 0.1519 - acc: 0.9706
# 1024/1750 [================>.............] - ETA: 0s - loss: 0.1348 - acc: 0.9746
# 1472/1750 [========================>.....] - ETA: 0s - loss: 0.1384 - acc: 0.9708
# 1750/1750 [==============================] - 0s - loss: 0.1400 - acc: 0.9703 - val_loss: 0.3218 - val_acc: 0.9040
#     Epoch 9/10
    
#   32/1750 [..............................] - ETA: 0s - loss: 0.0552 - acc: 1.0000
#  512/1750 [=======>......................] - ETA: 0s - loss: 0.0856 - acc: 0.9902
#  992/1750 [================>.............] - ETA: 0s - loss: 0.0999 - acc: 0.9879
# 1472/1750 [========================>.....] - ETA: 0s - loss: 0.1084 - acc: 0.9844
# 1750/1750 [==============================] - 0s - loss: 0.1151 - acc: 0.9800 - val_loss: 0.3379 - val_acc: 0.8933
#     Epoch 10/10
    
#   32/1750 [..............................] - ETA: 0s - loss: 0.1595 - acc: 0.9688
#  544/1750 [========>.....................] - ETA: 0s - loss: 0.1178 - acc: 0.9724
# 1088/1750 [=================>............] - ETA: 0s - loss: 0.1010 - acc: 0.9807
# 1632/1750 [==========================>...] - ETA: 0s - loss: 0.0997 - acc: 0.9804
# 1750/1750 [==============================] - 0s - loss: 0.0978 - acc: 0.9817 - val_loss: 0.3109 - val_acc: 0.9067


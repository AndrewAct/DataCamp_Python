# # 8/7/2020
# The trained network from your previous coding exercise is now stored as model. New data to make predictions is stored in a NumPy array as pred_data. Use model to make predictions on your new data.

# In this exercise, your predictions will be probabilities, which is the most common way for data scientists to communicate their predictions to colleagues.

# Specify, compile, and fit the model
model = Sequential()
model.add(Dense(32, activation='relu', input_shape = (n_cols,)))
model.add(Dense(2, activation='softmax'))
model.compile(optimizer='sgd', 
              loss='categorical_crossentropy', 
              metrics=['accuracy'])
model.fit(predictors, target)

# Calculate predictions: predictions
predictions = model.predict(pred_data)

# Calculate predicted probability of survival: predicted_prob_true
predicted_prob_true = predictions[:, 1]

# print predicted_prob_true
print(predicted_prob_true)

# <script.py> output:
#     Epoch 1/10
    
#  32/800 [>.............................] - ETA: 0s - loss: 5.3679 - acc: 0.3438
# 640/800 [=======================>......] - ETA: 0s - loss: 2.5157 - acc: 0.5781
# 800/800 [==============================] - 0s - loss: 2.3207 - acc: 0.5825     
#     Epoch 2/10
    
#  32/800 [>.............................] - ETA: 0s - loss: 2.9760 - acc: 0.3438
# 768/800 [===========================>..] - ETA: 0s - loss: 1.2286 - acc: 0.6055
# 800/800 [==============================] - 0s - loss: 1.2019 - acc: 0.6088     
#     Epoch 3/10
    
#  32/800 [>.............................] - ETA: 0s - loss: 0.6071 - acc: 0.7500
# 672/800 [========================>.....] - ETA: 0s - loss: 0.7360 - acc: 0.6652
# 800/800 [==============================] - 0s - loss: 0.7325 - acc: 0.6650     
#     Epoch 4/10
    
#  32/800 [>.............................] - ETA: 0s - loss: 0.5543 - acc: 0.6875
# 768/800 [===========================>..] - ETA: 0s - loss: 0.7503 - acc: 0.6628
# 800/800 [==============================] - 0s - loss: 0.7487 - acc: 0.6600     
#     Epoch 5/10
    
#  32/800 [>.............................] - ETA: 0s - loss: 0.5900 - acc: 0.6875
# 768/800 [===========================>..] - ETA: 0s - loss: 0.6176 - acc: 0.6797
# 800/800 [==============================] - 0s - loss: 0.6201 - acc: 0.6800     
#     Epoch 6/10
    
#  32/800 [>.............................] - ETA: 0s - loss: 0.6345 - acc: 0.6562
# 544/800 [===================>..........] - ETA: 0s - loss: 0.6224 - acc: 0.6893
# 800/800 [==============================] - 0s - loss: 0.6423 - acc: 0.6900     
#     Epoch 7/10
    
#  32/800 [>.............................] - ETA: 0s - loss: 0.5628 - acc: 0.6562
# 480/800 [=================>............] - ETA: 0s - loss: 0.6699 - acc: 0.6729
# 800/800 [==============================] - 0s - loss: 0.6485 - acc: 0.6875     
#     Epoch 8/10
    
#  32/800 [>.............................] - ETA: 0s - loss: 0.6174 - acc: 0.6250
# 576/800 [====================>.........] - ETA: 0s - loss: 0.6485 - acc: 0.6684
# 800/800 [==============================] - 0s - loss: 0.6453 - acc: 0.6737     
#     Epoch 9/10
    
#  32/800 [>.............................] - ETA: 0s - loss: 0.6685 - acc: 0.6562
# 608/800 [=====================>........] - ETA: 0s - loss: 0.6322 - acc: 0.6727
# 800/800 [==============================] - 0s - loss: 0.6195 - acc: 0.6863     
#     Epoch 10/10
    
#  32/800 [>.............................] - ETA: 0s - loss: 0.7258 - acc: 0.5938
# 640/800 [=======================>......] - ETA: 0s - loss: 0.6026 - acc: 0.6953
# 800/800 [==============================] - 0s - loss: 0.6127 - acc: 0.6863     
#     [0.26896247 0.43761036 0.82858837 0.53096086 0.23623177 0.20956866
#      0.09725156 0.36504236 0.215958   0.5770225  0.25979725 0.33288428
#      0.2162936  0.45033494 0.21540712 0.17831217 0.30380282 0.46751526
#      0.12777324 0.4474886  0.676713   0.26235938 0.10227494 0.36473075
#      0.47318584 0.21758355 0.5873161  0.56623363 0.22920243 0.58230287
#      0.47815725 0.48570126 0.2245136  0.29392648 0.3668022  0.689719
#      0.33484367 0.21627907 0.59390515 0.4665187  0.33265626 0.4135592
#      0.49734586 0.19114345 0.3882273  0.13693196 0.4445862  0.19582245
#      0.4778647  0.7614004  0.425027   0.03622145 0.4840157  0.60376257
#      0.29159367 0.41713867 0.9306064  0.254625   0.46345505 0.2245136
#      0.16753101 0.355179   0.2787018  0.4683735  0.36463663 0.19427982
#      0.3509561  0.56926733 0.23852883 0.45681092 0.25994387 0.47544363
#      0.18954277 0.11743512 0.4697994  0.42988396 0.37290293 0.34502494
#      0.21409863 0.6127147  0.48456132 0.19546212 0.36974117 0.29273915
#      0.25615612 0.5052865  0.33921114 0.5452211  0.42862478 0.48779067
#      0.21132195]